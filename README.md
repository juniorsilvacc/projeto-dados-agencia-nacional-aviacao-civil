# âœˆï¸ Projeto: Data Warehouse de AgÃªncia Nacional de AviaÃ§Ã£o Civil (ANAC)

Este projeto tem como objetivo construir um Data Warehouse da AgÃªncia Nacional de AviaÃ§Ã£o Civil (ANAC), utilizando nuvem e infraestrutura local. A soluÃ§Ã£o coleta, processa e armazena dados pÃºblicos da ANAC disponibilizados em arquivos CSV contendo informaÃ§Ãµes de voos, possibilitando anÃ¡lises e insights relevantes para Ã³rgÃ£os pÃºblicos, pesquisadores e interessados no setor aÃ©reo brasileiro.

# ğŸ¯ Objetivos do Projeto

- Extrair dados da ANAC em arquivos CSV disponibilizados periodicamente (a cada 60 ou 90 dias);
- Armazenar os dados em um banco de dados PostgreSQL;
- Processar e transformar os dados para anÃ¡lises futuras;
- Automatizar a ingestÃ£o dos dados utilizando Airflow;
- Fornecer insights e dashboards com base nos dados coletados.

# ğŸ”— Fonte dos Dados

- [Portal Brasileiro de Dados Abertos](https://dados.gov.br/dados/busca?termo=ANAC)
- [CatÃ¡logo de APIs Governamentais](https://www.gov.br/conecta/catalogo/)
- Outras bases pÃºblicas relevantes (quando necessÃ¡rio)

# ğŸ”§ Tecnologias Utilizadas

### Infraestrutura (em definiÃ§Ã£o)

- BigQuery (GCP) â€“ possÃ­vel escolha para armazenamento e anÃ¡lises escalÃ¡veis em nuvem
- Amazon S3 / Redshift (AWS) â€“ considerados para armazenamento de arquivos e data warehouse
- **Infraestrutura local** (Utilizada para backup e processamento complementar)
- **PostgreSQL** (Banco relacional local para ingestÃ£o inicial dos dados)

### Linguagens e Ferramentas

- **Python** (ExtraÃ§Ã£o e processamento)
- **Pandas** (Tratamento e anÃ¡lise dos dados)
- **Psycopg2** (ConexÃ£o com PostgreSQL)
- **Apache Airflow** (OrquestraÃ§Ã£o e agendamento de tarefas)
- **Streamlit** (Dashboard e visualizaÃ§Ã£o de dados)
- **Poetry** (Gerenciamento de dependÃªncias e ambiente virtual)

## ğŸ“ Estrutura do Projeto

```
ğŸ“‚ config/
â”‚ â”œâ”€â”€ db_config.py            # ConfiguraÃ§Ãµes e conexÃ£o com o PostgreSQL (ainda nÃ£o implementado)
ğŸ“‚ dags/
â”‚ â”œâ”€â”€ download_new_data.py    # DAGs do Airflow para orquestrar o pipeline de ingestÃ£o
ğŸ“‚ data/
â”‚ â”œâ”€â”€ raw/                    # Armazenamento dos arquivos CSV brutos
â”‚ â”œâ”€â”€ processed/              # Dados transformados e prontos para anÃ¡lise
â”‚ â””â”€â”€ processed.log           # Log de controle dos arquivos jÃ¡ processados
ğŸ“‚ src/
â”‚ â”œâ”€â”€ extract.py              # ExtraÃ§Ã£o dos dados dos CSVs
â”‚ â”œâ”€â”€ transform.py            # TransformaÃ§Ã£o e limpeza
â”‚ â””â”€â”€ load.py                 # Carregamento para banco de dados
ğŸ“‚ tests/
â”‚ â”œâ”€â”€ test_transform.py       # Testes automatizados da etapa de transformaÃ§Ã£o
â”‚
â”œâ”€â”€ .env                      # VariÃ¡veis de ambiente (nÃ£o versionado)
â”œâ”€â”€ .gitignore                # Arquivos/pastas ignorados pelo Git
â”œâ”€â”€ main.py                   # Script principal para rodar o pipeline
â”œâ”€â”€ poetry.lock               # VersÃµes travadas das dependÃªncias
â”œâ”€â”€ pyproject.toml            # ConfiguraÃ§Ã£o do projeto com Poetry
â””â”€â”€ README.md                 # DocumentaÃ§Ã£o do projeto
```

## ğŸš€ ImplementaÃ§Ã£o

### 1ï¸âƒ£ Requisitos
- Python 3.10+
- [Poetry](https://python-poetry.org/docs/) instalado globalmente
- PostgreSQL configurado e acessÃ­vel
- Airflow instalado (recomendado via Docker)
- VariÃ¡veis de ambiente configuradas em `.env`

### 2ï¸âƒ£ InicializaÃ§Ã£o do Ambiente
ğŸ§ª Utilizando Poetry (ExecuÃ§Ã£o Manual)
```bash
# Clone o repositÃ³rio
git clone https://github.com/juniorsilvacc/projeto-dados-agencia-nacional-aviacao-civil.git

# Acesse o diretÃ³rio do projeto
cd projeto-dados-agencia-nacional-aviacao-civil

# Instale as dependÃªncias com Poetry
poetry install

# Ative o ambiente virtual
poetry shell
```

ğŸ³ Utilizando Docker (ExecuÃ§Ã£o Automatizada)
Este projeto tambÃ©m pode ser executado dentro de um ambiente Dockerizado, ideal para manter consistÃªncia no ambiente e facilitar a configuraÃ§Ã£o.
```bash
# Crie um arquivo .env na raiz do projeto com as seguintes variÃ¡veis (exemplo):
DB_HOST=db
DB_PORT=5432
DB_NAME=db-projeto-dados-anac
DB_USER=postgres
DB_PASSWORD=postgres

# Vai reconstruir a imagem e mostrar os logs da execuÃ§Ã£o. Quando o script main.py acabar, o container vai parar manualmente
docker-compose up --build

# Para acompanhar os logs
docker-compose logs -f

# Executar o pipeline manualmente via docker
docker-compose run --rm app python main.py
```

### 3ï¸âƒ£ Executando o Pipeline Manualmente
```bash
python main.py
```

### 4ï¸âƒ£ Planejamento para IntegraÃ§Ã£o com o Airflow

A automaÃ§Ã£o da ingestÃ£o de dados estÃ¡ planejada para ser feita com **Apache Airflow**, permitindo o agendamento e orquestraÃ§Ã£o de todo o pipeline ETL.

ğŸš§ Esta etapa ainda estÃ¡ em desenvolvimento.

Etapas planejadas:
- Criar DAGs para download, transformaÃ§Ã£o e carga dos dados
- Usar agendamentos (schedule_interval) para execuÃ§Ã£o periÃ³dica
- Monitoramento do pipeline via interface do Airflow

ğŸ’¡ O uso de Docker ou ambiente virtual separado para o Airflow estÃ¡ sendo considerado.


### âœ… Testes
```bash
# Para rodar os testes:
pytest tests/
```

### ğŸ“Š Dashboard
```bash
# O projeto conta com um dashboard desenvolvido com Streamlit para visualizaÃ§Ã£o dos dados transformados:
streamlit run dashboard.py
```

### ğŸ“Œ ContribuiÃ§Ã£o
CÃ­cero JÃºnior (Engenheiro de Dados)
